<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=h, initial-scale=1.0">
    <title>Document</title>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async
          src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
    </script>
    <style>
     .img{
        width: 200;
        height: 100;
        margin-left: 0;
        margin-right: 0;
    }
     .body {
    font-family:Georgia, 'Times New Roman', Times, serif; 
    }
    .center {
    display: block;
    margin-left: auto;
    margin-right: auto;
    width: 50%;
    }
    </style>
</head>
<body>
    <img src="Sabanci_University_logo.png" width="200" height="100">
    <h1>Statistic Selection and MCMC for Differentially Private Bayesian Estimation</h1>
    <ul>
        <li><b>Project Title:</b> Data Privacy Applications with Monte Carlo Methods</li>
        <li><b>Principle Investigator:</b> Sinan Yıldırım</li>
        <li><b>Project Type:</b> TUBITAK 3501</li>
        <li><b>Team:</b>Sinan Yıldırım, Barış Alparslan</li>
      </ul> 
    <p><b>Target of the project:</b> Differentially private Bayesian estimation of the parameters of a population distribution, when a noisy statistic of a sample from that population is shared.</p>
    <p>This work mainly addresses two problems: (1) What statistic of the sample should be shared privately? For the first question, i.e., the one about statistic selection, we promote using the Fisher information. We find out that, the statistic that is most informative in a non-privacy setting may not be the optimal choice under the privacy restrictions. We provide several examples to support that point. We consider several types of data sharing settings and propose several Monte Carlo-based numerical estimation methods for calculating the Fisher information for those settings. The second question concerns inference: (2) Based on the shared statistics, how could we perform effective Bayesian inference?</p>
    <p><b>Main contributions:</b></p>
    <ul>
        <li>Statistic selection based on Fisher Information. </li>
        <li>Sampling from the true posterior based on the noisy statistic. </li>
    </ul>
    <p><b>Research Output:</b></p>
    <ul>
        <li><b>Submitted:</b> Statistic Selection and MCMC for Differentially Private Bayesian Estimation, B.Alparslan, S.Yıldırım</li>
        <li><b>Work in progress:</b>Differentially Private Linear Regression with Gibbs Sampling, B.Alparslan, S.Yıldırım</li>
    </ul>
    <h2>Statistic selection based on Fisher Information</h2>
    <figure>
        <img src="privacy_settings.png" width="100" height="200" class="center">
    </figure>
    <dl>
        <dt>(1)FIM with additive statistic and Gaussian noise</dt>
        <dd>-Additive statistic \(S_n(X_{1:n})=\frac{1}{n}\sum s(X_i)\)</dd>
        <dd>-Gaussian noise </dd>
        <dt>(2) FIM with additive statistic and non-Gaussian noise</dt>
        <dd>-Additive statistic \(S_n(X_{1:n})=\frac{1}{n}\sum s(X_i)\)</dd>
        <dd>-non-Gaussian noise(Laplace) </dd>
        <dt>(3) FIM with non-additive statistic and non-Gaussian noise</dt>
        <dd>-non-additive statistic \(S_n(X_{1:n})=\text{max}(s(X_i))\) or \(S_n(X_{1:n})=\text{median}(s(X_i))\)</dd>
        <dd>-non-Gaussian noise(Laplace) </dd>
        <dt>(4) FIM with sequential release</dt>
        <dd>-Instead of a summary statistic, sequential release is employed</dd>
        <dd>-non-Gaussian noise(Laplace) </dd>
    </dl>
    <h3>Bayesian Inference using MCMC</h3>
    MCMC algorithms used in the project
    <ul>
        <li><b>Simple Metropolis-Hastings(MH)</li>
        <li><b>Pseudo-Marginal MH</b></li>
        <li><b>MH with Averaged Acceptance Ratios</b></li>
    </ul>
</body>
</html>
